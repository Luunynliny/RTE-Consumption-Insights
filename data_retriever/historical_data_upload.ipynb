{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actual_generations_per_production_type import *\n",
    "from consolidated_power_consumption import *\n",
    "from weekly_forecasts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload test data on RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2021-01-01T00:00:00+01:00\"\n",
    "end_date = \"2021-02-01T00:00:00+01:00\"\n",
    "\n",
    "response = get_consolidated_production_type(start_date, end_date)\n",
    "df = table_production_type(response)\n",
    "df_clean = clean_data_production_type(df)\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(current_directory, 'data/production_type.csv')\n",
    "\n",
    "# Save df to the file and overwrite if it already exists\n",
    "df_clean.to_csv(file_path, mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "file_path1 = 'data/production_type.csv'\n",
    "file_path2 = 'data/power_consumption.csv'\n",
    "file_path3 = 'data/weekly_forecast.csv'\n",
    "\n",
    "# Define the data types for each column\n",
    "dtype1 = {'column1': int, 'column2': float, 'column3': str}  # Replace column1, column2, column3 with actual column names and their respective data types\n",
    "dtype2 = {'column1': int, 'column2': float, 'column3': str}  # Replace column1, column2, column3 with actual column names and their respective data types\n",
    "dtype3 = {'column1': int, 'column2': float, 'column3': str}  # Replace column1, column2, column3 with actual column names and their respective data types\n",
    "\n",
    "# Load the dataframes with specified data types\n",
    "df1 = pd.read_csv(file_path1, dtype=dtype1)\n",
    "df2 = pd.read_csv(file_path2, dtype=dtype2)\n",
    "df3 = pd.read_csv(file_path3, dtype=dtype3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create SQLAlchemy engine to connect to MySQL Database\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "                       .format(user=\"admin\",\n",
    "                               pw=\"admin2024\",\n",
    "                               host=\"mysql-database.c9cwko0aaypr.eu-central-1.rds.amazonaws.com\",\n",
    "                               db=\"rte\"))\n",
    "\n",
    "# Convert and upload the DataFrame to MySQL\n",
    "df1.to_sql('production_type', con = engine, if_exists = 'append', chunksize = 1000)\n",
    "df2.to_sql('power_consumption', con = engine, if_exists = 'append', chunksize = 1000)\n",
    "df3.to_sql('weekly_forecast', con = engine, if_exists = 'append', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
